{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17421,"status":"ok","timestamp":1713983677017,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"fVeWvYD28xSj","outputId":"e02eba8a-59e6-41db-faf4-e5286f2df3de"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m843.3/843.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.5/292.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/115.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q gigachat gigachain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kr1v8FWx7oH9"},"outputs":[],"source":["# Import things that are needed generically\n","from langchain.pydantic_v1 import BaseModel, Field\n","from langchain.tools import BaseTool, StructuredTool, tool"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4WaaMZjS8xj_"},"outputs":[],"source":["\n","from IPython.display import display\n","from IPython.display import Markdown"]},{"cell_type":"code","source":["def get_arxiv_papers(query):\n","    \"\"\"\n","    Searches Arxiv for papers related to the given query,\n","    extracts metadata and text content, and returns a list of dictionaries.\n","    \"\"\"\n","    try:\n","        arxiv = ArxivAPIWrapper()\n","        results = arxiv.run(query)\n","\n","        papers = []\n","        for result in results:\n","            # Parse the JSON string into a dictionary\n","            paper_dict = json.loads(result)\n","\n","            paper = {}\n","            paper['title'] = paper_dict.get('title')\n","            paper['authors'] = paper_dict.get('authors')\n","            # ... (rest of the code remains the same)\n","\n","        return papers\n","\n","    except Exception as e:\n","        print(f\"Error fetching or processing Arxiv papers: {e}\")\n","        return []"],"metadata":{"id":"yAfsd-JovN8O","executionInfo":{"status":"ok","timestamp":1713989548228,"user_tz":-180,"elapsed":458,"user":{"displayName":"Den Ma","userId":"02627840345123117167"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWfDm1Kp8xb7"},"outputs":[],"source":["\n","from langchain.chat_models.gigachat import GigaChat\n","\n","llm = GigaChat(credentials=\"NTlkY2MyZmItM2Q4ZC00ZWMzLWE2NjAtNTI3MzZhOTk2ZjQzOjVhZGJiZDQxLTc0YjAtNDQxNi04YjAzLTUxZDVmYTY4NTkwNw==\",\n","        #streaming=True,\n","        verify_ssl_certs=False,\n","        temperature= 0.0,\n","        max_tokens= 3000\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"executionInfo":{"elapsed":445,"status":"error","timestamp":1713964213973,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"UL6cyOlVOK0p","outputId":"dfed36cb-72d6-4348-e8c7-ae69c1d50c16"},"outputs":[{"ename":"TypeError","evalue":"'NoneType' object is not callable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-23789cd3eed6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"]}],"source":["llm.mode"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IOuILTemXb0z"},"outputs":[],"source":["from langchain_community.embeddings.gigachat import GigaChatEmbeddings\n","\n","embeddings = GigaChatEmbeddings(\n","    credentials=\"<авторизационные_данные>\", verify_ssl_certs=False\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vK1MUDM47X41"},"outputs":[],"source":["@tool\n","def search(query: str) -> str:\n","    \"\"\"Look up things online.\"\"\"\n","    return \"LangChain\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fg16u5XrNkcu"},"outputs":[],"source":["@tool\n","def multiply(a: int, b: int) -> int:\n","    \"\"\"Multiply two numbers.\"\"\"\n","    return a * b"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":878,"status":"ok","timestamp":1713865614654,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"VtW3rOTK7oRR","outputId":"a056c359-4dd0-44cf-e35e-44e13c0d0067"},"outputs":[{"name":"stdout","output_type":"stream","text":["search\n","search(query: str) -> str - Look up things online.\n","{'query': {'title': 'Query', 'type': 'string'}}\n"]}],"source":["print(search.name)\n","print(search.description)\n","print(search.args)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tO81Z37C7oZ8"},"outputs":[],"source":["class SearchInput(BaseModel):\n","    query: str = Field(description=\"should be a search query\")\n","\n","\n","@tool(\"search-tool\", args_schema=SearchInput, return_direct=True)\n","def search(query: str) -> str:\n","    \"\"\"Look up things online.\"\"\"\n","    return \"LangChain\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1713865665069,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"GF6QXLWj8e63","outputId":"a01b8893-e5f3-43fb-8d8c-b4b96fd2eaf9"},"outputs":[{"name":"stdout","output_type":"stream","text":["search-tool\n","search-tool(query: str) -> str - Look up things online.\n","{'query': {'title': 'Query', 'description': 'should be a search query', 'type': 'string'}}\n","True\n"]}],"source":["print(search.name)\n","print(search.description)\n","print(search.args)\n","print(search.return_direct)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vus9z-eK7oiN"},"outputs":[],"source":["from typing import Optional, Type\n","\n","from langchain.callbacks.manager import (\n","    AsyncCallbackManagerForToolRun,\n","    CallbackManagerForToolRun,\n",")\n","\n","\n","class SearchInput(BaseModel):\n","    query: str = Field(description=\"should be a search query\")\n","\n","\n","class CalculatorInput(BaseModel):\n","    a: int = Field(description=\"first number\")\n","    b: int = Field(description=\"second number\")\n","\n","\n","class CustomSearchTool(BaseTool):\n","    name = \"custom_search\"\n","    description = \"useful for when you need to answer questions about current events\"\n","    args_schema: Type[BaseModel] = SearchInput\n","\n","    def _run(\n","        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n","    ) -> str:\n","        \"\"\"Use the tool.\"\"\"\n","        return \"LangChain\"\n","\n","    async def _arun(\n","        self, query: str, run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n","    ) -> str:\n","        \"\"\"Use the tool asynchronously.\"\"\"\n","        raise NotImplementedError(\"custom_search does not support async\")\n","\n","\n","class CustomCalculatorTool(BaseTool):\n","    name = \"Calculator\"\n","    description = \"useful for when you need to answer questions about math\"\n","    args_schema: Type[BaseModel] = CalculatorInput\n","    return_direct: bool = True\n","\n","    def _run(\n","        self, a: int, b: int, run_manager: Optional[CallbackManagerForToolRun] = None\n","    ) -> str:\n","        \"\"\"Use the tool.\"\"\"\n","        return a * b\n","\n","    async def _arun(\n","        self,\n","        a: int,\n","        b: int,\n","        run_manager: Optional[AsyncCallbackManagerForToolRun] = None,\n","    ) -> str:\n","        \"\"\"Use the tool asynchronously.\"\"\"\n","        raise NotImplementedError(\"Calculator does not support async\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79NSfphU7oqC"},"outputs":[],"source":["from langchain_core.tools import ToolException\n","\n","\n","def search_tool1(s: str):\n","    raise ToolException(\"The search tool1 is not available.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tU9Ld9rV7ox6"},"outputs":[],"source":["from langchain.chains.summarize import load_summarize_chain\n","from langchain.chat_models.gigachat import GigaChat\n","from langchain.document_loaders import WebBaseLoader\n","\n","loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n","docs = loader.load()\n","\n","giga = GigaChat(user=\"<user_name>\", password=\"<password>\")\n","chain = load_summarize_chain(giga, chain_type=\"stuff\")\n","\n","chain.run(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10724,"status":"ok","timestamp":1713905886051,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"qAqAuemav6Af","outputId":"f2111234-6a19-4142-8134-73a356b53a82"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q wikipedia"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2216,"status":"ok","timestamp":1713905915521,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"gw0Sz_d2YVto","outputId":"489d1c59-e5d1-4e9e-95ec-edcf75de3dd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Parts count: 12\n"]}],"source":["from langchain.chains.summarize import load_summarize_chain\n","from langchain.chat_models.gigachat import GigaChat\n","from langchain.document_loaders import WikipediaLoader\n","from langchain.text_splitter import CharacterTextSplitter\n","\n","docs = WikipediaLoader(\n","    query=\"Винни-пух\", lang=\"ru\", load_max_docs=1, doc_content_chars_max=1000000\n",").load()\n","split_docs = CharacterTextSplitter(chunk_size=5000, chunk_overlap=500).split_documents(\n","    docs\n",")\n","print(f\"Parts count: {len(split_docs)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":303,"status":"ok","timestamp":1713906447273,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"VyQucNpwwRJW","outputId":"79568d0c-a255-42a2-a740-93c973d982e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Длина документа: 44413\n"]}],"source":["text=docs[0].page_content\n","print(f\"Длина документа: {len(text)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iy5WstIqYlOE"},"outputs":[],"source":["giga = GigaChat(profanity=False, credentials=...)\n","chain = load_summarize_chain(giga, chain_type=\"map_reduce\")\n","res = chain.run(split_docs)\n","\n","print(\"\\n\\n===\")\n","print(res)"]},{"cell_type":"markdown","metadata":{"id":"x7DWtuNTY9zf"},"source":["-------"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"elapsed":2437,"status":"ok","timestamp":1713906493273,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"JCoFHNSDY_Ph","outputId":"4ed5554d-495d-4a73-f223-49fd3231899d"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:langchain_community.chat_models.gigachat:Giga generation stopped with reason: blacklist\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Не люблю менять тему разговора, но вот сейчас тот самый случай.'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["chain = load_summarize_chain(llm, chain_type=\"refine\")\n","chain.run(docs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCF1LGozZIpr"},"outputs":[],"source":["\n","#refine\n","from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n","from langchain.chains.llm import LLMChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.prompts import PromptTemplate\n","\n","\n","\n","prompt_template = \"\"\"Write a concise summary of the following:\n","{text}\n","CONCISE SUMMARY:\"\"\"\n","prompt = PromptTemplate.from_template(prompt_template)\n","\n","refine_template = (\n","    \"Your job is to produce a final summary\\n\"\n","    \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"\n","    \"We have the opportunity to refine the existing summary\"\n","    \"(only if needed) with some more context below.\\n\"\n","    \"------------\\n\"\n","    \"{text}\\n\"\n","    \"------------\\n\"\n","    \"Given the new context, refine the original summary in Italian\"\n","    \"If the context isn't useful, return the original summary.\"\n",")\n","refine_prompt = PromptTemplate.from_template(refine_template)\n","chain = load_summarize_chain(\n","    llm=llm,\n","    chain_type=\"refine\",\n","    question_prompt=prompt,\n","    refine_prompt=refine_prompt,\n","    return_intermediate_steps=True,\n","    input_key=\"input_documents\",\n","    output_key=\"output_text\",\n",")\n","result = chain({\"input_documents\": split_docs}, return_only_outputs=True)"]},{"cell_type":"markdown","metadata":{"id":"6TE4A1Yd8bG1"},"source":["----------"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23275,"status":"ok","timestamp":1713907034916,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"rwcEV34g0NPu","outputId":"404a6fca-d14f-4c22-94ab-1cdac4dbe318"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q umap faiss-cpu wordcloud"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9rdHHKbj8c3S"},"outputs":[],"source":["#@title pre version\n","import networkx as nx\n","import plotly.graph_objects as go\n","from wordcloud import WordCloud\n","import matplotlib.pyplot as plt\n","from typing import Dict, List, Optional, Tuple\n","import time\n","import numpy as np\n","import pandas as pd\n","import umap\n","from langchain.prompts import ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser\n","from sklearn.mixture import GaussianMixture\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.docstore import InMemoryDocstore\n","import faiss\n","#from langchain_community.vectorstores import Croma\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import Chroma\n","RANDOM_SEED = 224  # Fixed seed for reproducibility\n","!pip install -q sentence-transformers\n","# Vectorizer\n","model_name = \"all-MiniLM-L6-v2\" #\"paraphrase-multilingual-mpnet-base-v2\"\n","model_kwargs = {'device': 'cpu'}\n","encode_kwargs = {'normalize_embeddings': False}\n","\n","embd = HuggingFaceEmbeddings(\n","    model_name=model_name,\n","    model_kwargs=model_kwargs,\n","    encode_kwargs=encode_kwargs\n","       )\n","embedding_size = 768\n","\n","#from datasets import load_dataset\n","#dataset = load_dataset(\"Denm/lch_codebase\")\n","#data_path=dataset[\"train\"][\"file_path\"]\n","#content=dataset[\"train\"][\"content\"]\n","\n","#from langchain.docstore.document import Document\n","#documents = [Document(page_content=text,metadata={\"source\":path}) for text,path in zip(content,data_path)]# Doc textsdocs.extend([*documents,*docs_pydantic, *docs_sq])\n","\n","docs_texts = [d.page_content for d in documents]\n","\n","# Doc texts split\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","\n","#chunk_size_tok = 200\n","#text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n","#    chunk_size=chunk_size_tok, chunk_overlap=0\n","#)\n","#texts_split = text_splitter.split_text(documents)\n","\n","def global_cluster_embeddings(\n","    embeddings: np.ndarray,\n","    dim: int,\n","    n_neighbors: Optional[int] = None,\n","    metric: str = \"cosine\",\n",") -> np.ndarray:\n","    \"\"\"\n","    Perform global dimensionality reduction on the embeddings using UMAP.\n","\n","    Parameters:\n","    - embeddings: The input embeddings as a numpy array.\n","    - dim: The target dimensionality for the reduced space.\n","    - n_neighbors: Optional; the number of neighbors to consider for each point.\n","                   If not provided, it defaults to the square root of the number of embeddings.\n","    - metric: The distance metric to use for UMAP.\n","\n","    Returns:\n","    - A numpy array of the embeddings reduced to the specified dimensionality.\n","    \"\"\"\n","    print(\"Perform global dimensionality reduction\")\n","    if n_neighbors is None:\n","        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n","    return umap.UMAP(\n","        n_neighbors=n_neighbors, n_components=dim, metric=metric\n","    ).fit_transform(embeddings)\n","\n","\n","def local_cluster_embeddings(\n","    embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",") -> np.ndarray:\n","    \"\"\"\n","    Perform local dimensionality reduction on the embeddings using UMAP, typically after global clustering.\n","\n","    Parameters:\n","    - embeddings: The input embeddings as a numpy array.\n","    - dim: The target dimensionality for the reduced space.\n","    - num_neighbors: The number of neighbors to consider for each point.\n","    - metric: The distance metric to use for UMAP.\n","\n","    Returns:\n","    - A numpy array of the embeddings reduced to the specified dimensionality.\n","    \"\"\"\n","    print(\"dimensionality reduction on the embeddings using UMAP, typically after global clustering.\")\n","    return umap.UMAP(\n","        n_neighbors=num_neighbors, n_components=dim, metric=metric\n","    ).fit_transform(embeddings)\n","\n","\n","def get_optimal_clusters(\n","    embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",") -> int:\n","    \"\"\"\n","    Determine the optimal number of clusters using the Bayesian Information Criterion (BIC) with a Gaussian Mixture Model.\n","\n","    Parameters:\n","    - embeddings: The input embeddings as a numpy array.\n","    - max_clusters: The maximum number of clusters to consider.\n","    - random_state: Seed for reproducibility.\n","\n","    Returns:\n","    - An integer representing the optimal number of clusters found.\n","    \"\"\"\n","    max_clusters = min(max_clusters, len(embeddings))\n","    n_clusters = np.arange(1, max_clusters)\n","    bics = []\n","    for n in n_clusters:\n","        gm = GaussianMixture(n_components=n, random_state=random_state)\n","        gm.fit(embeddings)\n","        bics.append(gm.bic(embeddings))\n","    return n_clusters[np.argmin(bics)]\n","\n","\n","def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n","    \"\"\"\n","    Cluster embeddings using a Gaussian Mixture Model (GMM) based on a probability threshold.\n","\n","    Parameters:\n","    - embeddings: The input embeddings as a numpy array.\n","    - threshold: The probability threshold for assigning an embedding to a cluster.\n","    - random_state: Seed for reproducibility.\n","\n","    Returns:\n","    - A tuple containing the cluster labels and the number of clusters determined.\n","    \"\"\"\n","    n_clusters = get_optimal_clusters(embeddings)\n","    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n","    gm.fit(embeddings)\n","    probs = gm.predict_proba(embeddings)\n","    labels = [np.where(prob > threshold)[0] for prob in probs]\n","    return labels, n_clusters\n","\n","\n","def perform_clustering(\n","    embeddings: np.ndarray,\n","    dim: int,\n","    threshold: float,\n",") -> List[np.ndarray]:\n","    \"\"\"\n","    Perform clustering on the embeddings by first reducing their dimensionality globally, then clustering\n","    using a Gaussian Mixture Model, and finally performing local clustering within each global cluster.\n","\n","    Parameters:\n","    - embeddings: The input embeddings as a numpy array.\n","    - dim: The target dimensionality for UMAP reduction.\n","    - threshold: The probability threshold for assigning an embedding to a cluster in GMM.\n","\n","    Returns:\n","    - A list of numpy arrays, where each array contains the cluster IDs for each embedding.\n","    \"\"\"\n","    if len(embeddings) <= dim + 1:\n","        # Avoid clustering when there's insufficient data\n","        return [np.array([0]) for _ in range(len(embeddings))]\n","\n","    # Global dimensionality reduction\n","    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n","    # Global clustering\n","    global_clusters, n_global_clusters = GMM_cluster(\n","        reduced_embeddings_global, threshold\n","    )\n","\n","    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n","    total_clusters = 0\n","\n","    # Iterate through each global cluster to perform local clustering\n","    for i in range(n_global_clusters):\n","        # Extract embeddings belonging to the current global cluster\n","        global_cluster_embeddings_ = embeddings[\n","            np.array([i in gc for gc in global_clusters])\n","        ]\n","\n","        if len(global_cluster_embeddings_) == 0:\n","            continue\n","        if len(global_cluster_embeddings_) <= dim + 1:\n","            # Handle small clusters with direct assignment\n","            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n","            n_local_clusters = 1\n","        else:\n","            # Local dimensionality reduction and clustering\n","            reduced_embeddings_local = local_cluster_embeddings(\n","                global_cluster_embeddings_, dim\n","            )\n","            local_clusters, n_local_clusters = GMM_cluster(\n","                reduced_embeddings_local, threshold\n","            )\n","\n","        # Assign local cluster IDs, adjusting for total clusters already processed\n","        for j in range(n_local_clusters):\n","            local_cluster_embeddings_ = global_cluster_embeddings_[\n","                np.array([j in lc for lc in local_clusters])\n","            ]\n","            indices = np.where(\n","                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n","            )[1]\n","            for idx in indices:\n","                all_local_clusters[idx] = np.append(\n","                    all_local_clusters[idx], j + total_clusters\n","                )\n","\n","        total_clusters += n_local_clusters\n","\n","    return all_local_clusters\n","\n","\n","\n","\n","def embed(texts):\n","    \"\"\"\n","    Generate embeddings for a list of text documents.\n","\n","    This function assumes the existence of an `embd` object with a method `embed_documents`\n","    that takes a list of texts and returns their embeddings.\n","\n","    Parameters:\n","    - texts: List[str], a list of text documents to be embedded.\n","\n","    Returns:\n","    - numpy.ndarray: An array of embeddings for the given text documents.\n","    \"\"\"\n","    text_embeddings = embd.embed_documents(texts)\n","    text_embeddings_np = np.array(text_embeddings)\n","    return text_embeddings_np\n","\n","\n","def embed_cluster_texts(texts):\n","    \"\"\"\n","    Embeds a list of texts and clusters them, returning a DataFrame with texts, their embeddings, and cluster labels.\n","\n","    This function combines embedding generation and clustering into a single step. It assumes the existence\n","    of a previously defined `perform_clustering` function that performs clustering on the embeddings.\n","\n","    Parameters:\n","    - texts: List[str], a list of text documents to be processed.\n","\n","    Returns:\n","    - pandas.DataFrame: A DataFrame containing the original texts, their embeddings, and the assigned cluster labels.\n","    \"\"\"\n","    text_embeddings_np = embed(texts)  # Generate embeddings\n","    cluster_labels = perform_clustering(\n","        text_embeddings_np, 10, 0.1\n","    )  # Perform clustering on the embeddings\n","    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n","    df[\"text\"] = texts  # Store original texts\n","    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n","    df[\"cluster\"] = cluster_labels  # Store cluster labels\n","    return df\n","\n","\n","def fmt_txt(df: pd.DataFrame) -> str:\n","    \"\"\"\n","    Formats the text documents in a DataFrame into a single string.\n","\n","    Parameters:\n","    - df: DataFrame containing the 'text' column with text documents to format.\n","\n","    Returns:\n","    - A single string where all text documents are joined by a specific delimiter.\n","    \"\"\"\n","    unique_txt = df[\"text\"].tolist()\n","    return \"--- --- \\n --- --- \".join(unique_txt)\n","\n","\n","def embed_cluster_summarize_texts(\n","    texts: List[str], level: int\n",") -> Tuple[pd.DataFrame, pd.DataFrame]:\n","    \"\"\"\n","    Embeds, clusters, and summarizes a list of texts. This function first generates embeddings for the texts,\n","    clusters them based on similarity, expands the cluster assignments for easier processing, and then summarizes\n","    the content within each cluster.\n","\n","    Parameters:\n","    - texts: A list of text documents to be processed.\n","    - level: An integer parameter that could define the depth or detail of processing.\n","\n","    Returns:\n","    - Tuple containing two DataFrames:\n","      1. The first DataFrame (`df_clusters`) includes the original texts, their embeddings, and cluster assignments.\n","      2. The second DataFrame (`df_summary`) contains summaries for each cluster, the specified level of detail,\n","         and the cluster identifiers.\n","    \"\"\"\n","\n","    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n","    df_clusters = embed_cluster_texts(texts)\n","\n","    # Prepare to expand the DataFrame for easier manipulation of clusters\n","    expanded_list = []\n","\n","    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n","    for index, row in df_clusters.iterrows():\n","        for cluster in row[\"cluster\"]:\n","            expanded_list.append(\n","                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n","            )\n","\n","    # Create a new DataFrame from the expanded list\n","    expanded_df = pd.DataFrame(expanded_list)\n","\n","    # Retrieve unique cluster identifiers for processing\n","    all_clusters = expanded_df[\"cluster\"].unique()\n","\n","    print(f\"--Generated {len(all_clusters)} clusters--\")\n","\n","    # Summarization\n","    template = \"\"\"Here is a sub-set of LangChain Expression Langauge doc.\n","\n","    LangChain Expression Langauge provides a way to compose chain in LangChain.\n","\n","    Give a detailed summary of the documentation provided.\n","\n","    Documentation:\n","    {context}\n","    \"\"\"\n","    prompt = ChatPromptTemplate.from_template(template)\n","    chain = prompt | model | StrOutputParser()\n","\n","    # Format text within each cluster for summarization\n","    summaries = []\n","    for i in all_clusters:\n","        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n","        formatted_txt = fmt_txt(df_cluster)\n","        time.sleep(50)\n","        summaries.append(chain.invoke({\"context\": formatted_txt}))\n","\n","    # Create a DataFrame to store summaries with their corresponding cluster and level\n","    df_summary = pd.DataFrame(\n","        {\n","            \"summaries\": summaries,\n","            \"level\": [level] * len(summaries),\n","            \"cluster\": list(all_clusters),\n","        }\n","    )\n","\n","    return df_clusters, df_summary\n","\n","\n","def recursive_embed_cluster_summarize(\n","    texts: List[str], level: int = 1, n_levels: int = 3\n",") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n","    \"\"\"\n","    Recursively embeds, clusters, and summarizes texts up to a specified level or until\n","    the number of unique clusters becomes 1, storing the results at each level.\n","\n","    Parameters:\n","    - texts: List[str], texts to be processed.\n","    - level: int, current recursion level (starts at 1).\n","    - n_levels: int, maximum depth of recursion.\n","\n","    Returns:\n","    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]], a dictionary where keys are the recursion\n","      levels and values are tuples containing the clusters DataFrame and summaries DataFrame at that level.\n","    \"\"\"\n","    results = {}  # Dictionary to store results at each level\n","\n","    # Perform embedding, clustering, and summarization for the current level\n","    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n","\n","    # Store the results of the current level\n","    results[level] = (df_clusters, df_summary)\n","\n","    # Determine if further recursion is possible and meaningful\n","    unique_clusters = df_summary[\"cluster\"].nunique()\n","    if level < n_levels and unique_clusters > 1:\n","        # Use summaries as the input texts for the next level of recursion\n","        new_texts = df_summary[\"summaries\"].tolist()\n","        next_level_results = recursive_embed_cluster_summarize(\n","            new_texts, level + 1, n_levels\n","        )\n","\n","        # Merge the results from the next level into the current results dictionary\n","        results.update(next_level_results)\n","\n","    return results\n","\n","def create_cluster_graph(results: Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]) -> nx.Graph:\n","    \"\"\"\n","    Create a NetworkX graph representation of the text clusters at different levels.\n","\n","    Parameters:\n","    - results: A dictionary where keys are the recursion levels, and values are tuples containing\n","        the clusters DataFrame and summaries DataFrame at that level.\n","\n","    Returns:\n","    - A NetworkX Graph object where each node represents a text document, and edges connect documents\n","        within the same cluster at different levels.\n","    - A dictionary where keys are tuples of node indices, and values are the level at which the edge was created.\n","    \"\"\"\n","    graph = nx.Graph()\n","    edge_levels = {}\n","\n","    # Add nodes for each text document\n","    for level, (df_clusters, _) in results.items():\n","        for index, row in df_clusters.iterrows():\n","            # Assuming your DataFrame has a column named 'text'\n","            graph.add_node(index, text=row['text'])\n","\n","    # Add edges between documents within the same cluster at each level\n","    for level, (df_clusters, _) in results.items():\n","        for index, row in df_clusters.iterrows():\n","            doc_clusters = row[\"cluster\"]\n","            for cluster in doc_clusters:\n","                cluster_docs = df_clusters[df_clusters[\"cluster\"].apply(lambda x: cluster in set(x))]\n","                for i, row1 in cluster_docs.iterrows():\n","                    for j, row2 in cluster_docs.iterrows():\n","                        if i != j:\n","                            graph.add_edge(i, j)\n","                            edge_levels[(i, j)] = level\n","\n","    # Add edge_levels nodes\n","    for (i, j), level in edge_levels.items():\n","        graph.add_node(f\"{i}_{j}\", level=level)\n","        graph.add_edge(i, f\"{i}_{j}\")\n","        graph.add_edge(j, f\"{i}_{j}\")\n","\n","    return graph, edge_levels\n","\n","def visualize_graph(graph, edge_levels):\n","    # Create a layout for the nodes\n","    pos = nx.spring_layout(graph)\n","\n","    # Create nodes\n","    node_x, node_y, node_text, node_colors = [], [], [], []\n","\n","    for node in graph.nodes(data=True):\n","        x, y = pos[node[0]]  # Use the calculated position\n","        node_x.append(x)\n","        node_y.append(y)\n","\n","        # Check if the node data has a 'text' attribute\n","        if 'text' in node[1]:\n","            node_text.append(f\"Node: {node[0]}<br>Text: {node[1]['text'][:50]}...\")\n","        else:\n","            # If 'text' is not present, use an empty string or handle it differently\n","            node_text.append(f\"Node: {node[0]}\")\n","\n","        node_colors.append('lightblue')\n","\n","    node_trace = go.Scatter(\n","        x=node_x,\n","        y=node_y,\n","        text=node_text,\n","        mode='markers',\n","        hoverinfo='text',\n","        marker=dict(\n","            color=node_colors,\n","            size=10,\n","            line_width=2\n","        )\n","    )\n","\n","    # Create edges\n","    edge_x, edge_y = [], []\n","\n","    for edge in graph.edges():\n","        x0, y0 = pos[edge[0]]\n","        x1, y1 = pos[edge[1]]\n","        edge_x.extend([x0, x1, None])\n","        edge_y.extend([y0, y1, None])\n","\n","    edge_trace = go.Scatter(\n","        x=edge_x,\n","        y=edge_y,\n","        mode='lines',\n","        line=dict(color='rgb(210,210,210)', width=1),\n","        hoverinfo='none'\n","    )\n","\n","    # Create edge_level nodes\n","    edge_level_x, edge_level_y, edge_level_text, edge_level_colors = [], [], [], []\n","\n","    for node in graph.nodes(data=True):\n","        if 'level' in node[1]:\n","            x, y = pos[node[0]]\n","            edge_level_x.append(x)\n","            edge_level_y.append(y)\n","            edge_level_text.append(f\"Edge Level: {node[1]['level']}\")\n","            edge_level_colors.append('red')\n","\n","    edge_level_trace = go.Scatter(\n","        x=edge_level_x,\n","        y=edge_level_y,\n","        text=edge_level_text,\n","        mode='markers',\n","        hoverinfo='text',\n","        marker=dict(\n","            color=edge_level_colors,\n","            size=20,\n","            line_width=2\n","        )\n","    )\n","\n","    # Create the figure\n","    fig = go.Figure(data=[edge_trace, node_trace, edge_level_trace],\n","                    layout=go.Layout(\n","                        title='Interactive Graph Visualization',\n","                        showlegend=False,\n","                        hovermode='closest',\n","                        margin=dict(b=20, l=5, r=5, t=40),\n","                        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n","                        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n","                    )\n","                   )\n","\n","    # Display the figure\n","    fig.show()\n","\n","def generate_word_cloud(graph):\n","    # Combine all text data from nodes into a single string\n","    all_text = ' '.join(node[1].get('text', '') for node in graph.nodes(data=True))\n","\n","    # Generate the word cloud\n","    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n","\n","    # Display the word cloud\n","    plt.figure(figsize=(10, 5))\n","    plt.imshow(wordcloud, interpolation='bilinear')\n","    plt.axis(\"off\")\n","    plt.show()\n","\n","# Example usage\n","# Assuming you have the 'results' dictionary with the required data structure\n","# Build tree\n","leaf_texts = text #docs_texts[:100]\n","results = recursive_embed_cluster_summarize(leaf_texts, level=5, n_levels=50)\n","print(results.keys())\n","cluster_graph, edge_levels = create_cluster_graph(results)\n","visualize_graph(cluster_graph, edge_levels)\n","generate_word_cloud(cluster_graph)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S3fttCplAyj-"},"outputs":[],"source":["metadata_dict = {\n","\n","\"title\": \"Заголовок или название текстового документа.\",\n","\"author\": \"Личность(и) или организация(и), создавшие текстовый документ.\",\n","\"publication_date\": \"Дата публикации или выхода в свет текстового документа.\",\n","\"publication_source\": \"Источник или издатель текстового документа (например, газета, журнал, веб-сайт).\",\n","\"document_type\": \"Тип текстового документа (например, новостная статья, научная работа, пост в социальных сетях).\",\n","\"subject\": \"Основная тема или темы, рассматриваемые в текстовом документе.\",\n","\"keywords\": \"Набор ключевых слов или тегов, описывающих содержание текстового документа.\",\n","\"language\": \"Язык, на котором написан текстовый документ.\",\n","\"sentiment\": \"Общий эмоциональный тон или настроение, выраженное в текстовом документе (например, положительный, отрицательный, нейтральный).\",\n","\"named entities\": \"Упомянутые в текстовом документе именованные сущности (например, люди, организации, местоположения).\",\n","\"text_domain\": \"Домен или отраслевой контекст текстового документа (например, финансы, здравоохранение, технологии).\",\n","\"audience\": \"Целевая аудитория или предполагаемые читатели текстового документа.\",\n","\"access_rights\": \"Ограничения доступа или лицензионная информация для текстового документа.\",\n","\"provenance\": \"История и источник текстового документа, включая любые предыдущие версии или модификации.\"\n","\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FD_2Aha-LRY5"},"outputs":[],"source":["from langchain_core.prompts import PromptTemplate\n","\n","REFINE_PROMPT_TMPL = (\n","    \"Твоя задача - создать окончательное резюме\\n\"\n","    \"Мы предоставили существующее резюме до определенного момента: {existing_answer}\\n\"\n","    \"У нас есть возможность улучшить существующее резюме\"\n","    \"(только если это необходимо) с некоторым дополнительным контекстом ниже.\\n\"\n","    \"------------\\n\"\n","    \"{text}\\n\"\n","    \"------------\\n\"\n","    \"Учитывая новый контекст, улучши оригинальное резюме\\n\"\n","    \"Если контекст не полезен, верни оригинальное резюме.\"\n",")\n","REFINE_PROMPT = PromptTemplate(\n","    input_variables=[\"existing_answer\", \"text\"],\n","    template=REFINE_PROMPT_TMPL,\n",")\n","\n","\n","prompt_template = \"\"\"Напиши краткое резюме следующего:\n","\n","\n","\"{text}\"\n","\n","\n","КРАТКОЕ РЕЗЮМЕ:\"\"\"\n","PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RM4Rt-KZR5lH"},"outputs":[],"source":["from typing import List, Optional\n","from pydantic import BaseModel\n","\n","class NamedEntity(BaseModel):\n","   name: str = None\n","   type: str = None\n","\n","class TextMetadata(BaseModel):\n","   title: str = None\n","   author: str = None\n","   publication_date: str = None\n","   publication_source: str = None\n","   document_type: str = None\n","   subject: List[str] = None\n","   keywords: List[str] = None\n","   language: str = None\n","   named_entities: List[NamedEntity] = None\n","   text_domain: str = None\n","   audience: str = None\n","   provenance: str = None"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3804,"status":"ok","timestamp":1713908295045,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"JTTNY0T95IIl","outputId":"a55f22a6-29fd-4823-e048-84edae2cbb8c"},"outputs":[{"data":{"text/plain":["AIMessage(content='Извините, но как виртуальный помощник, я не могу предоставить вам актуальную информацию о времени в реальном времени. Моя основная функция - предоставлять информацию и помощь на основе данных, которые у меня есть. Если вам нужна информация о текущем времени, лучше всего проверить часы или использовать онлайн-ресурсы, такие как Google или Yahoo, чтобы узнать точное время.', response_metadata={'token_usage': Usage(prompt_tokens=14, completion_tokens=80, total_tokens=94), 'model_name': 'GigaChat:3.1.24.3', 'finish_reason': 'stop'}, id='run-111666a1-5001-4cbc-ba21-82e67950633f-0')"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["model.invoke(\"сколько сейчас время?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ukmi6jq-iQLR"},"outputs":[],"source":["from typing import List, Optional\n","\n","class Tags(BaseModel):\n","\n","    language: str = Field(\n","           ...,\n","           enum=[\"русский\", \"испанский\", \"английский\", \"французский\", \"немецкий\", \"итальянский\"]\n","           )\n","\n","    topic: str = Field (\n","           ...,\n","           description=\"к какой категории это относится?\",\n","           enum=[\n","           \"Политика\",\n","           \"Бизнес и экономика\",\n","           \"Наука и технологии\",\n","           \"Здоровье и медицина\",\n","           \"Образование\",\n","           \"Окружающая среда и изменение климата\",\n","           \"Спорт\",\n","           \"Развлечения\",\n","           \"Мировые новости\",\n","           \"Преступность и правосудие\",\n","           \"Образ жизни и мода\",\n","           \"Путешествия и туризм\",\n","           \"Еда и гастрономия\",\n","           \"Искусство и культура\",\n","           \"Автомобильная промышленность\",\n","           \"Недвижимость\",\n","           \"Социальные вопросы\",\n","           \"Погода\",\n","           \"Религия и духовность\",\n","           \"Гейминг и киберспорт\",\n","           \"Технологические тренды\",\n","           \"Финансовые рынки\",\n","           \"Спортивный бизнес\",\n","           \"Новости о знаменитостях\",\n","           \"Научная фантастика и фэнтези\",\n","           \"Социальные сети и блогеры\",\n","           \"Потребительская электроника\",\n","           \"Устойчивое развитие и экологический образ жизни\",\n","           \"Психическое здоровье\",\n","           \"Индустрия моды\",\n","           \"Международное развитие\",\n","           \"Автомобильные технологии\",\n","           \"Поп-культура\",\n","           \"Родительство и семья\",\n","           \"Криптовалюты и блокчейн\",\n","           \"Авиация\",\n","           \"Кибербезопасность и конфиденциальность\",\n","           \"Социальная справедливость\",\n","           \"Дизайн интерьера\",\n","           \"Образовательные технологии\"\n","          ])\n","\n","class TextSummary(BaseModel):\n","    summaries: List[str] = Field(\n","        ..., description=\"\"\"\n","Определите центральную тему или основную идею текста.\n","Зафиксируйте наиболее важные ключевые моменты, факты и аргументы, опуская мелкие детали.\n","Кратко перефразируйте контент своими словами, обычно от четверти до одной трети исходной длины.\n","Сохраняйте объективность, избегая личных мнений и суждений.\n","Излагайте ключевые моменты логично и связно, следуя исходной структуре.\n","Используйте ясный и простой язык без ненужных сложностей.\n","Источники атрибутов при обобщении нескольких текстов.\n","Включите заключительное предложение, подчеркивающее главный вывод.\n","Просмотрите и исправьте резюме на предмет точности, полноты и ясности.\n","\n","\"\"\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fK9-aQTD391j"},"outputs":[],"source":["from langchain.globals import set_verbose\n","\n","set_verbose(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wUyq2CpUlwVr"},"outputs":[],"source":["\n","from langchain.output_parsers import PydanticOutputParser\n","from langchain_core.prompts import PromptTemplate\n","\n","def get_tags(text: str) -> Tags:\n","    parser = PydanticOutputParser(pydantic_object=Tags)\n","    prompt = PromptTemplate(\n","        template=\"Ответьте на запрос пользователя, если ответ не знаете, напишите 'None'.\\n{format_instructions}\\n{query}\\n\",\n","        input_variables=[\"query\"],\n","        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n","    )\n","    print(dir(prompt))\n","    chain = prompt | llm | parser\n","    return chain.invoke({\"query\": {text}})\n","\n","def get_summaries(text: str) ->TextSummary:\n","    parser = PydanticOutputParser(pydantic_object=TextSummary)\n","\n","    prompt = PromptTemplate(\n","        template=\"Ответьте на запрос пользователя.\\n{format_instructions}\\n{query}\\n\",\n","        input_variables=[\"query\"],\n","        partial_variables={\"format_instructions\": parser.get_format_instructions()},\n","    )\n","    chain = prompt | llm | parser\n","    return chain.invoke({\"query\": {text}})"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":862},"executionInfo":{"elapsed":2469,"status":"error","timestamp":1713908445195,"user":{"displayName":"Den Ma","userId":"02627840345123117167"},"user_tz":-180},"id":"WS0mCUXTvhoJ","outputId":"ba4fea1c-5ecc-488a-a728-5f707f72ac30"},"outputs":[{"name":"stdout","output_type":"stream","text":["['Config', 'InputType', 'OutputType', '__abstractmethods__', '__add__', '__annotations__', '__class__', '__class_getitem__', '__class_vars__', '__config__', '__custom_root_type__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__exclude_fields__', '__fields__', '__fields_set__', '__format__', '__ge__', '__get_validators__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__include_fields__', '__init__', '__init_subclass__', '__iter__', '__json_encoder__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__or__', '__orig_bases__', '__parameters__', '__post_root_validators__', '__pre_root_validators__', '__pretty__', '__private_attributes__', '__reduce__', '__reduce_ex__', '__repr__', '__repr_args__', '__repr_name__', '__repr_str__', '__rich_repr__', '__ror__', '__schema_cache__', '__setattr__', '__setstate__', '__signature__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__try_update_forward_refs__', '__validators__', '__weakref__', '_abatch_with_config', '_abc_impl', '_acall_with_config', '_aformat_prompt_with_error_handling', '_atransform_stream_with_config', '_batch_with_config', '_calculate_keys', '_call_with_config', '_copy_and_set_values', '_decompose_class', '_enforce_dict_if_root', '_format_prompt_with_error_handling', '_get_value', '_init_private_attributes', '_is_protocol', '_iter', '_lc_kwargs', '_merge_partial_and_user_variables', '_prompt_type', '_transform_stream_with_config', '_validate_input', 'abatch', 'abatch_as_completed', 'aformat', 'aformat_prompt', 'ainvoke', 'assign', 'astream', 'astream_events', 'astream_log', 'atransform', 'batch', 'batch_as_completed', 'bind', 'config_schema', 'config_specs', 'configurable_alternatives', 'configurable_fields', 'construct', 'copy', 'dict', 'format', 'format_prompt', 'from_examples', 'from_file', 'from_orm', 'from_template', 'get_graph', 'get_input_schema', 'get_lc_namespace', 'get_name', 'get_output_schema', 'get_prompts', 'input_schema', 'input_types', 'input_variables', 'invoke', 'is_lc_serializable', 'json', 'lc_attributes', 'lc_id', 'lc_secrets', 'map', 'metadata', 'name', 'output_parser', 'output_schema', 'parse_file', 'parse_obj', 'parse_raw', 'partial', 'partial_variables', 'pick', 'pipe', 'pretty_print', 'pretty_repr', 'save', 'schema', 'schema_json', 'stream', 'tags', 'template', 'template_format', 'template_is_valid', 'to_json', 'to_json_not_implemented', 'transform', 'update_forward_refs', 'validate', 'validate_template', 'validate_variable_names', 'with_config', 'with_fallbacks', 'with_listeners', 'with_retry', 'with_types']\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:langchain_community.chat_models.gigachat:Giga generation stopped with reason: blacklist\n"]},{"ename":"OutputParserException","evalue":"Invalid json output: Как у нейросетевой языковой модели у меня не может быть настроения, но почему-то я совсем не хочу говорить на эту тему.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mparse_json_markdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36mparse_json_markdown\u001b[0;34m(json_string, parser)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mjson_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parse_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36m_parse_json\u001b[0;34m(json_str, parser)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;31m# Parse the JSON string into a Python dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/utils/json.py\u001b[0m in \u001b[0;36mparse_partial_json\u001b[0;34m(s, strict)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;31m# for the original string.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0mkw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'parse_constant'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOutputParserException\u001b[0m                     Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-25ccd22ab6dc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-45-5b8d07b0dfb1>\u001b[0m in \u001b[0;36mget_tags\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprompt\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"query\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_summaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m\u001b[0mTextSummary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2491\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m                 input = step.invoke(\n\u001b[0m\u001b[1;32m   2494\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m                     \u001b[0;31m# mark each step as a child run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    167\u001b[0m     ) -> T:\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m    170\u001b[0m                 lambda inner_input: self.parse_result(\n\u001b[1;32m    171\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mChatGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1617\u001b[0m             output = cast(\n\u001b[1;32m   1618\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1619\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1620\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/base.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(inner_input)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseMessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             return self._call_with_config(\n\u001b[0;32m--> 170\u001b[0;31m                 lambda inner_input: self.parse_result(\n\u001b[0m\u001b[1;32m    171\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mChatGeneration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 ),\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/pydantic.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mGeneration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     ) -> TBaseModel:\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mjson_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/output_parsers/json.py\u001b[0m in \u001b[0;36mparse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Invalid json output: {text}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mOutputParserException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mllm_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutputParserException\u001b[0m: Invalid json output: Как у нейросетевой языковой модели у меня не может быть настроения, но почему-то я совсем не хочу говорить на эту тему."]}],"source":["get_tags(text)"]},{"cell_type":"markdown","metadata":{"id":"sZb_IeKYw0Ln"},"source":["-------------"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Gh4isHZwy77"},"outputs":[],"source":["\n","#baseline snippet\n","from typing import List, Optional, Union\n","from langchain.agents import initialize_agent, Tool\n","from langchain.agents.tools import ToolSelectionAgent\n","from langchain.agents.agent_toolkits import NotepadsAgent\n","from langchain.tools import BaseTool\n","\n","# TextSummarizationTool\n","desc = \"Use this tool to summarize a given text. Provide the 'text' parameter with the content you want to summarize.\"\n","\n","class TextSummarizationTool(BaseTool):\n","    name = \"Text Summarization\"\n","    description = desc\n","\n","    def _run(self, text: str) -> str:\n","        # Use a summarization model or library to summarize the given text\n","        # and return the summary\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# CodeCompletionTool\n","desc = \"Use this tool to complete or generate code snippets. Provide the 'code' parameter with the initial code, 'language' for the programming language, and 'task' with a description of what you want the code to do.\"\n","\n","class CodeCompletionTool(BaseTool):\n","    name = \"Code Completion\"\n","    description = desc\n","\n","    def _run(self, code: str, language: str, task: str) -> str:\n","        # Use a code completion model or library to generate the code snippet\n","        # based on the initial code, language, and task description\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# TranslationTool\n","desc = \"Use this tool to translate text between languages. Provide the 'text' parameter with the content to translate, 'source_lang' for the original language, and 'target_lang' for the desired language.\"\n","\n","class TranslationTool(BaseTool):\n","    name = \"Translation\"\n","    description = desc\n","\n","    def _run(self, text: str, source_lang: str, target_lang: str) -> str:\n","        # Use a translation model or library to translate the given text\n","        # from the source language to the target language\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# ImageCaptioningTool\n","desc = \"Use this tool to generate a caption for an image. Provide the 'image' parameter with the image file or URL.\"\n","\n","class ImageCaptioningTool(BaseTool):\n","    name = \"Image Captioning\"\n","    description = desc\n","\n","    def _run(self, image: Union[str, bytes]) -> str:\n","        # Use an image captioning model or library to generate a caption\n","        # for the given image\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# WebSearchTool\n","desc = \"Use this tool to perform web searches. Provide the 'query' parameter with the search term or question.\"\n","\n","class WebSearchTool(BaseTool):\n","    name = \"Web Search\"\n","    description = desc\n","\n","    def _run(self, query: str) -> str:\n","        # Use a web search engine or API to search for the given query\n","        # and return the search results\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# MathSolverTool\n","desc = \"Use this tool to solve mathematical expressions or word problems. Provide the 'expression' parameter with the math problem.\"\n","\n","class MathSolverTool(BaseTool):\n","    name = \"Math Solver\"\n","    description = desc\n","\n","    def _run(self, expression: str) -> str:\n","        # Use a math solver library or API to solve the given expression\n","        # or word problem and return the solution\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# DataVisualizationTool\n","desc = \"Use this tool to generate visualizations (charts, plots, etc.) from data. Provide the 'data' parameter with the dataset and 'visualization_type' for the desired chart type.\"\n","\n","class DataVisualizationTool(BaseTool):\n","    name = \"Data Visualization\"\n","    description = desc\n","\n","    def _run(self, data: Union[str, dict, list], visualization_type: str) -> str:\n","        # Use a data visualization library or API to generate a visualization\n","        # based on the given data and desired chart type\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# SentimentAnalysisTool\n","desc = \"Use this tool to analyze the sentiment (positive, negative, or neutral) of a given text. Provide the 'text' parameter with the content to analyze.\"\n","\n","class SentimentAnalysisTool(BaseTool):\n","    name = \"Sentiment Analysis\"\n","    description = desc\n","\n","    def _run(self, text: str) -> str:\n","        # Use a sentiment analysis model or library to determine the sentiment\n","        # of the given text and return the result\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# EntityExtractionTool\n","desc = \"Use this tool to extract named entities (people, organizations, locations, etc.) from a given text. Provide the 'text' parameter with the content to analyze.\"\n","\n","class EntityExtractionTool(BaseTool):\n","    name = \"Entity Extraction\"\n","    description = desc\n","\n","    def _run(self, text: str) -> str:\n","        # Use a named entity recognition model or library to extract entities\n","        # from the given text and return the results\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# TextGenerationTool\n","desc = \"Use this tool to generate text based on a given prompt or context. Provide the 'prompt' parameter with the initial text or topic.\"\n","\n","class TextGenerationTool(BaseTool):\n","    name = \"Text Generation\"\n","    description = desc\n","\n","    def _run(self, prompt: str) -> str:\n","        # Use a text generation model or library to generate text\n","        # based on the given prompt or context\n","        ...\n","\n","    def _arun(self, query: str):\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","# Create a list of tools\n","tools: List[Tool] = [\n","    TextSummarizationTool(),\n","    CodeCompletionTool(),\n","    TranslationTool(),\n","    ImageCaptioningTool(),\n","    WebSearchTool(),\n","    MathSolverTool(),\n","    DataVisualizationTool(),\n","    SentimentAnalysisTool(),\n","    EntityExtractionTool(),\n","    TextGenerationTool(),\n","]\n","\n","# Create the ToolSelectionAgent\n","tool_selection_agent = ToolSelectionAgent(tools=tools)\n","\n","# Create the NotepadsAgent\n","notepads_agent = NotepadsAgent(tool_selection_agent=tool_selection_agent)\n","\n","# Run the agent and provide input\n","query = \"Your query or task here\"\n","result = notepads_agent.run(query)\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6uw9K868C1w7","outputId":"239ee6d2-449e-4717-e24d-3807cafd7030"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q duckduckgo-search faiss-cpu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4CaOeprC1_-"},"outputs":[],"source":["# Memory\n","import faiss\n","from langchain.docstore import InMemoryDocstore\n","from langchain_community.vectorstores import FAISS\n","#from langchain_openai import OpenAIEmbeddings\n","from langchain_google_genai import GoogleGenerativeAIEmbeddings\n","from langchain.vectorstores import Chroma\n","\n","embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\",maxRetries=15,request_timeout=100,task_type=\"retrieval_document\")\n","#embeddings_model = OpenAIEmbeddings()\n","embedding_size = 768\n","index = faiss.IndexFlatL2(embedding_size)\n","vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ldp8pIy8Dw7n","colab":{"base_uri":"https://localhost:8080/","height":335},"executionInfo":{"status":"error","timestamp":1713983635969,"user_tz":-180,"elapsed":24092,"user":{"displayName":"Den Ma","userId":"02627840345123117167"}},"outputId":"14e80bc5-5180-40fc-8db6-cc1ec7ffb5ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]},{"output_type":"error","ename":"NameError","evalue":"name 'DuckDuckGoSearchRun' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b95f876d8d6b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q duckduckgo_search'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mweb_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDuckDuckGoSearchRun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'DuckDuckGoSearchRun' is not defined"]}],"source":["!pip install -q duckduckgo_search\n","web_search = DuckDuckGoSearchRun()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3dbzvOdgEnO9"},"outputs":[],"source":["from langchain.chains.qa_with_sources.loading import (\n","    BaseCombineDocumentsChain,\n","    load_qa_with_sources_chain,\n",")\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.tools import BaseTool, DuckDuckGoSearchRun\n","from pydantic import Field\n","\n","\n","def _get_text_splitter():\n","    return RecursiveCharacterTextSplitter(\n","        # Set a really small chunk size, just to show.\n","        chunk_size=500,\n","        chunk_overlap=20,\n","        length_function=len,\n","    )\n","\n","\n","class WebpageQATool(BaseTool):\n","    name = \"query_webpage\"\n","    description = (\n","        \"Browse a webpage and retrieve the information relevant to the question.\"\n","    )\n","    text_splitter: RecursiveCharacterTextSplitter = Field(\n","        default_factory=_get_text_splitter\n","    )\n","    qa_chain: BaseCombineDocumentsChain\n","\n","    def _run(self, url: str, question: str) -> str:\n","        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n","        result = browse_web_page.run(url)\n","        docs = [Document(page_content=result, metadata={\"source\": url})]\n","        web_docs = self.text_splitter.split_documents(docs)\n","        results = []\n","        # TODO: Handle this with a MapReduceChain\n","        for i in range(0, len(web_docs), 4):\n","            input_docs = web_docs[i : i + 4]\n","            window_result = self.qa_chain(\n","                {\"input_documents\": input_docs, \"question\": question},\n","                return_only_outputs=True,\n","            )\n","            results.append(f\"Response from window {i} - {window_result}\")\n","        results_docs = [\n","            Document(page_content=\"\\n\".join(results), metadata={\"source\": url})\n","        ]\n","        return self.qa_chain(\n","            {\"input_documents\": results_docs, \"question\": question},\n","            return_only_outputs=True,\n","        )\n","\n","    async def _arun(self, url: str, question: str) -> str:\n","        raise NotImplementedError"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N234pKg2ElXl"},"outputs":[],"source":["query_website_tool = WebpageQATool(qa_chain=load_qa_with_sources_chain(llm))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3EENmKoDcJD"},"outputs":[],"source":["tools = [\n","    web_search,\n","    WriteFileTool(root_dir=\"./data\"),\n","    ReadFileTool(root_dir=\"./data\"),\n","    process_csv,\n","    query_website_tool,\n","    # HumanInputRun(), # Activate if you want the permit asking for help from the human\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Am7Y_XiDEHRi"},"outputs":[],"source":["agent.run(\n","    [\n","        \"Составь прогноз погоды в Крыму с 23 января по 27 января\"\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vNjSZ-bTMsGU"},"outputs":[],"source":["from typing import Any, Dict, List\n","from langchain.chains import RefineChain\n","from langchain.chains.combine_documents.base import BaseCombineDocumentsChain\n","from langchain.docstore.document import Document\n","from langchain.prompts import PromptTemplate\n","from langchain.tools import BaseTool\n","\n","# Define the prompts for the refine steps\n","initial_prompt_template = PromptTemplate(\n","    input_variables=[\"page_content\", \"question\"],\n","    template=\"Given the following webpage content:\\n{page_content}\\n\\nAnswer the question: {question}\",\n",")\n","\n","refine_prompt_template = PromptTemplate(\n","    input_variables=[\"summaries\", \"question\"],\n","    template=\"Given the following summaries from different parts of a webpage:\\n{summaries}\\n\\nAnswer the question: {question}\",\n","    refine_template=\"{refined_result}\"\n",")\n","\n","class RefineTool(BaseTool):\n","    name = \"refine_webpage_qa\"\n","    description = \"Use this tool to refine the answer to a question based on the content of a webpage.\"\n","\n","    def __init__(self, qa_model, combine_documents_chain):\n","        super().__init__()\n","        self.refine_chain = RefineChain.from_prompts(\n","            initial_prompt_template,\n","            refine_prompt_template,\n","            qa_model,\n","            combine_documents_chain=combine_documents_chain,\n","        )\n","\n","    def _run(self, url: str, question: str, text_splitter) -> str:\n","        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n","        result = browse_web_page.run(url)\n","        docs = [Document(page_content=result, metadata={\"source\": url})]\n","        web_docs = text_splitter.split_documents(docs)\n","\n","        return self.refine_chain({\"input_documents\": web_docs, \"question\": question})\n","\n","    async def _arun(self, url: str, question: str, text_splitter) -> str:\n","        raise NotImplementedError\n","\n","class WebpageQATool(BaseTool):\n","    name = \"query_webpage\"\n","    description = (\n","        \"Browse a webpage and retrieve the information relevant to the question.\"\n","    )\n","    text_splitter: RecursiveCharacterTextSplitter = Field(\n","        default_factory=_get_text_splitter\n","    )\n","    refine_tool: RefineTool\n","\n","    def __init__(self, qa_model, combine_documents_chain):\n","        super().__init__()\n","        self.refine_tool = RefineTool(qa_model, combine_documents_chain)\n","\n","    def _run(self, url: str, question: str) -> str:\n","        return self.refine_tool._run(url, question, self.text_splitter)\n","\n","    async def _arun(self, url: str, question: str) -> str:\n","        raise NotImplementedError"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AC-GNu7jLChZ"},"outputs":[],"source":["from typing import Union\n","from langchain.tools import BaseTool\n","import math\n","\n","class MathSolverTool(BaseTool):\n","    name = \"Math Solver\"\n","    description = \"A tool to solve various mathematical problems. Supported operations: solve_equation, simplify_expression, find_derivative, find_integral, and calculate_value.\"\n","\n","    def _run(self, query: str) -> str:\n","        \"\"\"Run the math solver tool on the given query.\"\"\"\n","        try:\n","            operation, expression = query.split(maxsplit=1)\n","            operation = operation.strip().lower()\n","\n","            if operation == \"solve_equation\":\n","                result = self.solve_equation(expression)\n","            elif operation == \"simplify_expression\":\n","                result = self.simplify_expression(expression)\n","            elif operation == \"find_derivative\":\n","                result = self.find_derivative(expression)\n","            elif operation == \"find_integral\":\n","                result = self.find_integral(expression)\n","            elif operation == \"calculate_value\":\n","                result = self.calculate_value(expression)\n","            else:\n","                return f\"Invalid operation: '{operation}'. Supported operations: solve_equation, simplify_expression, find_derivative, find_integral, and calculate_value.\"\n","\n","            return str(result)\n","        except Exception as e:\n","            return f\"Error: {str(e)}\"\n","\n","    def _arun(self, query: str) -> str:\n","        raise NotImplementedError(\"This tool does not support async\")\n","\n","    @staticmethod\n","    def solve_equation(equation: str) -> Union[float, str]:\n","        \"\"\"Solve the given equation.\"\"\"\n","        try:\n","            # Implement your equation-solving logic here\n","            return 0.0\n","        except Exception as e:\n","            return str(e)\n","\n","    @staticmethod\n","    def simplify_expression(expression: str) -> Union[str, str]:\n","        \"\"\"Simplify the given expression.\"\"\"\n","        try:\n","            # Implement your expression simplification logic here\n","            return expression\n","        except Exception as e:\n","            return str(e)\n","\n","    @staticmethod\n","    def find_derivative(expression: str) -> Union[str, str]:\n","        \"\"\"Find the derivative of the given expression.\"\"\"\n","        try:\n","            # Implement your derivative calculation logic here\n","            return expression\n","        except Exception as e:\n","            return str(e)\n","\n","    @staticmethod\n","    def find_integral(expression: str) -> Union[str, str]:\n","        \"\"\"Find the indefinite integral of the given expression.\"\"\"\n","        try:\n","            # Implement your indefinite integral calculation logic here\n","            return expression\n","        except Exception as e:\n","            return str(e)\n","\n","    @staticmethod\n","    def calculate_value(expression: str) -> Union[float, str]:\n","        \"\"\"Calculate the value of the given expression.\"\"\"\n","        try:\n","            return eval(expression)\n","        except Exception as e:\n","            return str(e)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JyEiYpFpN3sh"},"outputs":[],"source":["from typing import Any, Dict, List\n","from langchain.chains.combine_documents.refine import RefineDocumentsChain\n","from langchain.chains.combine_documents.base import BaseCombineDocumentsChain\n","from langchain.docstore.document import Document\n","from langchain.prompts import PromptTemplate\n","from langchain.tools import BaseTool\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.chains.qa_with_sources.loading import (\n","    BaseCombineDocumentsChain,\n","    load_qa_with_sources_chain,\n",")\n","\n","# Define the prompts for the refine steps\n","initial_prompt_template = PromptTemplate(\n","    input_variables=[\"page_content\", \"question\"],\n","    template=\"Given the following webpage content:\\n{page_content}\\n\\nAnswer the question: {question}\",\n",")\n","\n","refine_prompt_template = PromptTemplate(\n","    input_variables=[\"summaries\", \"question\"],\n","    template=\"Given the following summaries from different parts of a webpage:\\n{summaries}\\n\\nAnswer the question: {question}\",\n","    refine_template=\"{refined_result}\"\n",")\n","\n","def _get_text_splitter():\n","    return RecursiveCharacterTextSplitter(\n","        # Set a really small chunk size, just to show.\n","        chunk_size=500,\n","        chunk_overlap=20,\n","        length_function=len,\n","    )\n","\n","class RefineTool(BaseTool):\n","    name = \"summaries_webpage_qa\"\n","    description = \"Use this tool to summaries the answer to a question based on the content of a webpage.\"\n","\n","    def __init__(self, qa_model, combine_documents_chain):\n","        super().__init__()\n","        self.refine_chain = RefineDocumentsChain.from_prompts(\n","            initial_prompt_template,\n","            refine_prompt_template,\n","            qa_model,\n","            combine_documents_chain=combine_documents_chain,\n","        )\n","\n","    def _run(self, url: str, question: str, text_splitter) -> str:\n","        \"\"\"Useful for browsing websites and scraping the text information.\"\"\"\n","        result = browse_web_page.run(url)\n","        docs = [Document(page_content=result, metadata={\"source\": url})]\n","        web_docs = text_splitter.split_documents(docs)\n","\n","        return self.refine_chain({\"input_documents\": web_docs, \"question\": question})\n","\n","    async def _arun(self, url: str, question: str, text_splitter) -> str:\n","        raise NotImplementedError\n","\n","\n","class WebpageQATool(BaseTool):\n","    name = \"query_webpage\"\n","    description = (\n","        \"Browse a webpage and retrieve the information relevant to the question.\"\n","    )\n","    text_splitter: RecursiveCharacterTextSplitter = Field(\n","        default_factory=_get_text_splitter\n","    )\n","    refine_tool: RefineTool\n","\n","    def __init__(self, qa_model, combine_documents_chain):\n","        super().__init__()\n","        self.refine_tool = RefineTool(qa_model, combine_documents_chain)\n","\n","    def _run(self, url: str, question: str) -> str:\n","        return self.refine_tool._run(url, question, self.text_splitter)\n","\n","    async def _arun(self, url: str, question: str) -> str:\n","        raise NotImplementedError"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvHfxpcmda4x"},"outputs":[],"source":["refine_tool_instance = RefineTool()\n","lwebpage_qa_tool = WebpageQATool(refine_tool=refine_tool_instance)\n","\n","# Test data setup (mock URLs, questions, and expected answers)\n","test_cases = [\n","     {\"url\": \"https://www.pinecone.io/learn/series/langchain/langchain-tools/\", \"question\": \"What is main topic about?\", \"expected_answer\": \"Summaries answer for: What is topic about?\"},\n","     {\"url\": \"https://www.pinecone.io/learn/series/langchain/langchain-tools/\", \"question\": \"summaries page\", \"expected_answer\": \"summaries answer for: How does topic2 work?\"},\n","     # Add more test cases as needed\n","  ]\n","\n","# Evaluation\n","correct_answers = 0\n","for case in test_cases:\n","    actual_answer = webpage_qa_tool.process_webpage(case[\"url\"], case[\"question\"])\n","    if actual_answer == case[\"expected_answer\"]:\n","        print(f\"Test passed for: {case['question']}\")\n","        correct_answers += 1\n","    else:\n","        print(f\"Test failed for: {case['question']}\\nExpected: {case['expected_answer']}\\nGot: {actual_answer}\")\n","\n","# Summary\n","total_tests = len(test_cases)\n","print(f\"\\nSummary: {correct_answers}/{total_tests} correct answers.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoWtkv-zLEoC"},"outputs":[],"source":["MathSolverTool("]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SROu6OMpWIcU"},"outputs":[],"source":["from langchain_community.tools import DuckDuckGoSearchRun\n","\n","search_tool = DuckDuckGoSearchRun()\n","tools = [search_tool]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Nqm0ctq0WFa5","outputId":"4ed5921e-3047-4147-b455-4a224f1a0de3"},"outputs":[{"ename":"NameError","evalue":"name 'giga' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-81d6249103e9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgentExecutor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_gigachat_functions_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_gigachat_functions_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgiga\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#AgentExecutor создает среду, в которой будет работать агент\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'giga' is not defined"]}],"source":["from langchain.agents import AgentExecutor, create_gigachat_functions_agent\n","\n","agent = create_gigachat_functions_agent(giga, tools)\n","\n","#AgentExecutor создает среду, в которой будет работать агент\n","agent_executor = AgentExecutor(\n","    agent=agent,\n","    tools=tools,\n","    verbose=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XO9_9EwWLnA"},"outputs":[],"source":["from langchain.chat_models.gigachat import GigaChat\n","\n","giga = GigaChat(credentials=\"NTlkY2MyZmItM2Q4ZC00ZWMzLWE2NjAtNTI3MzZhOTk2ZjQzOjVhZGJiZDQxLTc0YjAtNDQxNi04YjAzLTUxZDVmYTY4NTkwNw==\", model=<модель_с_поддержкой_функций>)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f56UkcN-WQkR"},"outputs":[],"source":["agent_executor.invoke(\n","    {\"input\": \"Найди текущий курс биткоина и напечатай только число\"}\n",")[\"output\"]"]},{"cell_type":"code","source":["!pip install -q arxiv sentence-transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aclHvJ3jjEcR","executionInfo":{"status":"ok","timestamp":1713986521720,"user_tz":-180,"elapsed":93919,"user":{"displayName":"Den Ma","userId":"02627840345123117167"}},"outputId":"dae8999f-d3cc-4ecd-8a81-d93e4fc63653"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/171.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/171.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["import nest_asyncio\n","nest_asyncio.apply()"],"metadata":{"id":"z_nKQ2Deng4m","executionInfo":{"status":"ok","timestamp":1713987530299,"user_tz":-180,"elapsed":301,"user":{"displayName":"Den Ma","userId":"02627840345123117167"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","execution_count":16,"metadata":{"id":"ciJkBaKpWQ3U","executionInfo":{"status":"ok","timestamp":1713987534921,"user_tz":-180,"elapsed":2390,"user":{"displayName":"Den Ma","userId":"02627840345123117167"}}},"outputs":[],"source":["import asyncio\n","import logging\n","import nest_asyncio\n","nest_asyncio.apply()\n","\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.embeddings import SentenceTransformerEmbeddings\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.tools import ArxivQueryRun\n","from langchain.vectorstores import FAISS\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","\n","# Initialize tools\n","arxiv_query = ArxivQueryRun()\n","embeddings = SentenceTransformerEmbeddings()\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","\n","\n","async def search_arxiv(query: str) -> list[dict]:\n","    \"\"\"Searches Arxiv for relevant papers and extracts information.\"\"\"\n","    # Fetch papers from Arxiv\n","    papers = await arxiv_query.arun(query=query, max_results=5)\n","\n","    # Process each paper\n","    results = []\n","    for paper in papers:\n","        try:\n","            # Load PDF content\n","            pdf_loader = PyPDFLoader(paper[\"pdf_url\"])\n","            pdf_text = pdf_loader.load_and_split()[0].page_content\n","\n","            # Split text into chunks\n","            chunks = text_splitter.split_text(pdf_text)\n","\n","            # Create embeddings for chunks\n","            embeddings_chunks = embeddings.embed_documents(chunks)\n","\n","            # Store embeddings in a vector store for semantic search\n","            vectorstore = FAISS.from_documents(chunks, embeddings_chunks)\n","\n","            results.append(\n","                {\n","                    \"title\": paper[\"title\"],\n","                    \"abstract\": paper[\"summary\"],\n","                    \"vectorstore\": vectorstore,\n","                }\n","            )\n","        except Exception as e:\n","            logging.error(f\"Error processing paper: {e}\")\n","\n","    return results"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"ZPh5f5vrWRPI","executionInfo":{"status":"ok","timestamp":1713987539352,"user_tz":-180,"elapsed":287,"user":{"displayName":"Den Ma","userId":"02627840345123117167"}}},"outputs":[],"source":["result=search_arxiv(query=\"transformers layers\")"]},{"cell_type":"code","source":["dir(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WpfN1OyIkAwF","executionInfo":{"status":"ok","timestamp":1713987558227,"user_tz":-180,"elapsed":361,"user":{"displayName":"Den Ma","userId":"02627840345123117167"}},"outputId":"5be92f6d-563d-4103-ac13-dcc900252d2e"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__await__',\n"," '__class__',\n"," '__del__',\n"," '__delattr__',\n"," '__dir__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattribute__',\n"," '__gt__',\n"," '__hash__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__le__',\n"," '__lt__',\n"," '__name__',\n"," '__ne__',\n"," '__new__',\n"," '__qualname__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__setattr__',\n"," '__sizeof__',\n"," '__str__',\n"," '__subclasshook__',\n"," 'close',\n"," 'cr_await',\n"," 'cr_code',\n"," 'cr_frame',\n"," 'cr_origin',\n"," 'cr_running',\n"," 'send',\n"," 'throw']"]},"metadata":{},"execution_count":19}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJLCaeauSgjTJX+/GgBK2k"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}